# Arbeitsblatt 5: Benchmarking und Evaluation von LLMs

## Theorie-Teil: Benchmarking und Evaluation
- Benchmarks sind Datensätze, die zeigen, wie gut ein Modell in einer Aufgabe abschneidet.
- Bekannte Benchmarks: GLUE, SuperGLUE, SQuAD, MMLU, HELM.

## Aufgabe 5: Praxisaufgabe - Mini-Benchmark erstellen

### Teilaufgabe A:
1. Suche den Datensatz „MMLU“ (Massive Multitask Language Understanding) und analysiere die Aufgaben darin.

### Teilaufgabe B:
1. Erstelle eine Mini-Evaluation mit 5-10 Fragen.
2. Sende diese Fragen an unterschiedliche Modelle (GPT-3.5, GPT-4 etc.) über die API.
3. Dokumentiere und vergleiche die Resultate in einer kleinen Tabelle.
